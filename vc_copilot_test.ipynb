{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VC Copilot Backend Demo\n",
    "\n",
    "This notebook demonstrates the backend functionality of the VC Copilot, showing:\n",
    "1. Website data collection\n",
    "2. Executive Summary generation\n",
    "3. Success Prediction\n",
    "\n",
    "Let's start by importing the necessary modules and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing /founders ===\n",
      "{'founders': [{'achievements': ['Building an AI insurance company'],\n",
      "               'age': 'Not publicly listed',\n",
      "               'education': [{'degree': 'Not available',\n",
      "                              'institution': 'Not available',\n",
      "                              'year': 'Not available'}],\n",
      "               'expertise': ['AI', 'Insurance'],\n",
      "               'location': 'San Francisco, CA, USA',\n",
      "               'name': 'Nico Laqua',\n",
      "               'previous_startups': [],\n",
      "               'social_profiles': {'linkedin': 'Not available',\n",
      "                                   'twitter': 'https://twitter.com/NicoLaqua'},\n",
      "               'title': 'CEO/CTO',\n",
      "               'work_experience': [{'company': 'Corgi',\n",
      "                                    'duration': '2024-present',\n",
      "                                    'position': 'CEO/CTO'}]},\n",
      "              {'achievements': ['Dropped out of Stanford as a CS major'],\n",
      "               'age': 'Not publicly listed',\n",
      "               'education': [{'degree': 'Not available',\n",
      "                              'institution': 'Stanford University',\n",
      "                              'year': 'Not available'}],\n",
      "               'expertise': ['AI', 'Insurance'],\n",
      "               'location': 'San Francisco, CA, USA',\n",
      "               'name': 'Emily Yuan',\n",
      "               'previous_startups': [],\n",
      "               'social_profiles': {'linkedin': 'Not available',\n",
      "                                   'twitter': 'Not available'},\n",
      "               'title': 'Co-Founder and COO',\n",
      "               'work_experience': [{'company': 'Corgi',\n",
      "                                    'duration': '2024-present',\n",
      "                                    'position': 'Co-Founder and COO'}]}],\n",
      " 'founding_story': 'Nico Laqua and Emily Yuan met while working on their '\n",
      "                   'respective projects and came together to build Corgi, an '\n",
      "                   'AI insurance company. They were both passionate about '\n",
      "                   'leveraging AI to transform the insurance industry and '\n",
      "                   'decided to co-found Corgi in 2024.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "\n",
    "# API configuration\n",
    "BASE_URL = \"http://localhost:8000\"  # Update if your API runs on a different port\n",
    "\n",
    "# Test startup URL\n",
    "TEST_URL = \"https://corgi.insure\"  # Example URL, replace with your test case\n",
    "#https://telli.com\n",
    "#https://godela.ai\n",
    "#https://corgi.insure\n",
    "\n",
    "# Test /founders endpoint\n",
    "print(\"\\n=== Testing /founders ===\")\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/founders\",\n",
    "    params={\"url\": TEST_URL}\n",
    ")\n",
    "pprint(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing /scrape ===\n",
      "{'about_page': None,\n",
      " 'company_name': 'Corgi Insurance',\n",
      " 'contact_info': None,\n",
      " 'description': '',\n",
      " 'news_data': None,\n",
      " 'pages_scraped': [{'title': 'Corgi Insurance',\n",
      "                    'type': 'main',\n",
      "                    'url': 'https://corgi.insure',\n",
      "                    'word_count': 137}],\n",
      " 'products_services': None,\n",
      " 'raw_text': '--- MAIN PAGE ---\\n'\n",
      "             'Building the First Insurance Company Made possible with Join Us '\n",
      "             'Help Corgi reinvent insurance from scratch All Roles Leadership '\n",
      "             'Sales Engineering Deputy Chief of Staff K - K San Francisco / '\n",
      "             'Salt Lake City / New York Apply Strategic Projects K - K San '\n",
      "             'Francisco, CA, US Apply Ex-AE K - K Salt Lake City, UT, US Apply '\n",
      "             'GTM K - K Salt Lake City, UT, US Apply Senior SDR/BDR K - K Salt '\n",
      "             'Lake City, UT, US Apply Ex-BDR/SDR K - K Salt Lake City, UT, US '\n",
      "             'Apply Sales Development Representative K - K Salt Lake City, UT, '\n",
      "             'US Apply Business Development Representative K - K Salt Lake '\n",
      "             'City, UT, US Apply Heof Finance K - K San Francisco, CA, US '\n",
      "             'Apply Engineering Summer Intern K - K San Francisco, CA, US '\n",
      "             'Apply 1 2 ›',\n",
      " 'social_links': None,\n",
      " 'team_info': None,\n",
      " 'total_pages_found': 0}\n",
      "\n",
      "=== Testing /funding-news ===\n",
      "{'company_name': 'Corgi',\n",
      " 'funding_rounds': [{'amount': 'Not Available',\n",
      "                     'date': 'Not Available',\n",
      "                     'lead_investors': ['Not Available'],\n",
      "                     'other_investors': ['Not Available'],\n",
      "                     'round_type': 'Not Available',\n",
      "                     'source_url': 'Not Available',\n",
      "                     'valuation': 'Not Available'}],\n",
      " 'funding_status': 'Not Available',\n",
      " 'last_funding_date': 'Not Available',\n",
      " 'latest_news': [{'category': 'Not Available',\n",
      "                  'date': 'Not Available',\n",
      "                  'source': 'Not Available',\n",
      "                  'summary': 'Not Available',\n",
      "                  'title': 'Not Available',\n",
      "                  'url': 'Not Available'}],\n",
      " 'notable_investors': ['Not Available'],\n",
      " 'total_funding': 'Not Available',\n",
      " 'url': 'https://corgi.insure'}\n",
      "\n",
      "=== Testing /analyze ===\n",
      "{'company_name': 'Corgi Insurance',\n",
      " 'deep_dive_sections': {'Business Model': 'The website does not provide '\n",
      "                                          \"information about Corgi Insurance's \"\n",
      "                                          'revenue model, go-to-market '\n",
      "                                          'strategy, pricing, customer '\n",
      "                                          'acquisition details, or insights on '\n",
      "                                          'contract values or monetization '\n",
      "                                          'approach.',\n",
      "                        'Competition': 'Not enough information provided to '\n",
      "                                       'analyze this area thoroughly.',\n",
      "                        'Conclusion': 'Corgi Insurance is a company with a '\n",
      "                                      'vision to reinvent the insurance '\n",
      "                                      'industry. Despite the lack of detailed '\n",
      "                                      'information available on the website '\n",
      "                                      'and in the supplementary data, the '\n",
      "                                      \"company's active hiring across multiple \"\n",
      "                                      'locations suggests a growth phase. '\n",
      "                                      'However, without more specific '\n",
      "                                      'information about their product, '\n",
      "                                      'market, competition, and financial '\n",
      "                                      'status, a comprehensive assessment of '\n",
      "                                      \"the company's future prospects is not \"\n",
      "                                      'possible.',\n",
      "                        'Executive Summary': 'Corgi Insurance is a startup '\n",
      "                                             'focused on reinventing the '\n",
      "                                             'insurance industry. The company '\n",
      "                                             'has a strong presence in '\n",
      "                                             'multiple locations, including '\n",
      "                                             'San Francisco, Salt Lake City, '\n",
      "                                             \"and New York. The company's \"\n",
      "                                             'website lists a variety of job '\n",
      "                                             'roles, indicating a diverse team '\n",
      "                                             'structure. However, the website '\n",
      "                                             'does not provide detailed '\n",
      "                                             'background information about the '\n",
      "                                             \"company's history, how it got \"\n",
      "                                             'started, its initial '\n",
      "                                             'product-market fit, or its '\n",
      "                                             'evolution.',\n",
      "                        'Funding and Investors': 'The supplementary data '\n",
      "                                                 'suggests that detailed '\n",
      "                                                 'information about Corgi '\n",
      "                                                 \"Insurance's financial \"\n",
      "                                                 'backing, investment history, '\n",
      "                                                 'funding rounds, headcount '\n",
      "                                                 'growth, expansion plans, and '\n",
      "                                                 'IPO prospects is not '\n",
      "                                                 'available.',\n",
      "                        'Key Insights': 'Corgi Insurance is an ambitious '\n",
      "                                        'startup aiming to revolutionize the '\n",
      "                                        'insurance industry. They are actively '\n",
      "                                        'hiring for a wide range of roles, '\n",
      "                                        'suggesting that they are in a growth '\n",
      "                                        \"phase. The company's operations span \"\n",
      "                                        'multiple locations, indicating a '\n",
      "                                        'broad geographical reach. However, '\n",
      "                                        'the website does not provide specific '\n",
      "                                        \"details about the company's unique \"\n",
      "                                        'differentiators, AI integration, '\n",
      "                                        'expansion plans, or strategic '\n",
      "                                        'positioning.',\n",
      "                        'Key Risks': 'Not enough information provided to '\n",
      "                                     'analyze this area thoroughly.',\n",
      "                        'Problem & Market': \"Corgi Insurance's mission is to \"\n",
      "                                            'reinvent insurance from scratch, '\n",
      "                                            'suggesting that the company '\n",
      "                                            'identifies a problem with current '\n",
      "                                            'insurance offerings in the '\n",
      "                                            'market. However, the website does '\n",
      "                                            'not provide details about the '\n",
      "                                            'specific market opportunity, '\n",
      "                                            'target roles, verticals, '\n",
      "                                            'industries, or functions they '\n",
      "                                            'serve.',\n",
      "                        'Solution & Product': 'Corgi Insurance is building an '\n",
      "                                              'insurance company, but the '\n",
      "                                              'website does not provide '\n",
      "                                              'specific details about their '\n",
      "                                              'product offerings, their value '\n",
      "                                              'proposition, or who their '\n",
      "                                              'target users are.',\n",
      "                        'Team Info': 'The website does not provide specific '\n",
      "                                     'information about the founders, their '\n",
      "                                     'backgrounds, how they met, or their '\n",
      "                                     'experience. There is also no detailed '\n",
      "                                     'information about the current leadership '\n",
      "                                     'structure or the composition of the '\n",
      "                                     'team.',\n",
      "                        'Traction': 'Not enough information provided to '\n",
      "                                    'analyze this area thoroughly.'},\n",
      " 'founder_data': [],\n",
      " 'founder_evaluation': None,\n",
      " 'founding_story': None,\n",
      " 'funding_data': {'company_name': 'Corgi Insurance',\n",
      "                  'funding_rounds': [{'amount': 'Not Available',\n",
      "                                      'date': 'Not Available',\n",
      "                                      'lead_investors': ['Not Available'],\n",
      "                                      'other_investors': ['Not Available'],\n",
      "                                      'round_type': 'Not Available',\n",
      "                                      'source_url': 'Not Available',\n",
      "                                      'valuation': 'Not Available'}],\n",
      "                  'funding_status': 'Not Available',\n",
      "                  'last_funding_date': 'Not Available',\n",
      "                  'latest_news': [{'category': 'Not Available',\n",
      "                                   'date': 'Not Available',\n",
      "                                   'source': 'Not Available',\n",
      "                                   'summary': 'Not Available',\n",
      "                                   'title': 'Not Available',\n",
      "                                   'url': 'Not Available'},\n",
      "                                  {'category': 'Not Available',\n",
      "                                   'date': 'Not Available',\n",
      "                                   'source': 'Not Available',\n",
      "                                   'summary': 'Not Available',\n",
      "                                   'title': 'Not Available',\n",
      "                                   'url': 'Not Available'},\n",
      "                                  {'category': 'Not Available',\n",
      "                                   'date': 'Not Available',\n",
      "                                   'source': 'Not Available',\n",
      "                                   'summary': 'Not Available',\n",
      "                                   'title': 'Not Available',\n",
      "                                   'url': 'Not Available'}],\n",
      "                  'notable_investors': ['Not Available'],\n",
      "                  'total_funding': 'Not Available',\n",
      "                  'url': 'https://corgi.insure/'},\n",
      " 'url': 'https://corgi.insure/',\n",
      " 'website_data': {'about_page': None,\n",
      "                  'company_name': 'Corgi Insurance',\n",
      "                  'contact_info': None,\n",
      "                  'description': '',\n",
      "                  'news_data': None,\n",
      "                  'pages_scraped': [{'title': 'Corgi Insurance',\n",
      "                                     'type': 'main',\n",
      "                                     'url': 'https://corgi.insure/',\n",
      "                                     'word_count': 137}],\n",
      "                  'products_services': None,\n",
      "                  'raw_text': '--- MAIN PAGE ---\\n'\n",
      "                              'Building the First Insurance Company Made '\n",
      "                              'possible with Join Us Help Corgi reinvent '\n",
      "                              'insurance from scratch All Roles Leadership '\n",
      "                              'Sales Engineering Deputy Chief of Staff K - K '\n",
      "                              'San Francisco / Salt Lake City / New York Apply '\n",
      "                              'Strategic Projects K - K San Francisco, CA, US '\n",
      "                              'Apply Ex-AE K - K Salt Lake City, UT, US Apply '\n",
      "                              'GTM K - K Salt Lake City, UT, US Apply Senior '\n",
      "                              'SDR/BDR K - K Salt Lake City, UT, US Apply '\n",
      "                              'Ex-BDR/SDR K - K Salt Lake City, UT, US Apply '\n",
      "                              'Sales Development Representative K - K Salt '\n",
      "                              'Lake City, UT, US Apply Business Development '\n",
      "                              'Representative K - K Salt Lake City, UT, US '\n",
      "                              'Apply Heof Finance K - K San Francisco, CA, US '\n",
      "                              'Apply Engineering Summer Intern K - K San '\n",
      "                              'Francisco, CA, US Apply 1 2 ›',\n",
      "                  'social_links': None,\n",
      "                  'team_info': None,\n",
      "                  'total_pages_found': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test /scrape endpoint\n",
    "print(\"\\n=== Testing /scrape ===\")\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/scrape\",\n",
    "    params={\"url\": TEST_URL, \"max_pages\": 3}\n",
    ")\n",
    "pprint(response.json())\n",
    "\n",
    "\n",
    "\n",
    "# Test /funding-news endpoint\n",
    "print(\"\\n=== Testing /funding-news ===\")\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/funding-news\",\n",
    "    params={\"url\": TEST_URL}\n",
    ")\n",
    "pprint(response.json())\n",
    "\n",
    "# Test /analyze endpoint\n",
    "print(\"\\n=== Testing /analyze ===\")\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/analyze\",\n",
    "    json={\n",
    "        \"url\": TEST_URL,\n",
    "        \"data_sources\": [\"website\", \"founders\", \"funding_news\"],\n",
    "        \"analysis_types\": [\"deep_dive\", \"founder_evaluation\"],\n",
    "        \"max_pages\": 3\n",
    "    }\n",
    ")\n",
    "pprint(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great function for scraping. Problem with dynamic content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import logging\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from urllib.parse import urlparse, urljoin, urlunparse\n",
    "from typing import Optional, Dict, Any, List, Set, Tuple\n",
    "from pydantic import BaseModel\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ScrapedData(BaseModel):\n",
    "    company_name: str\n",
    "    description: str\n",
    "    raw_text: str\n",
    "    team_info: Optional[List[Dict[str, str]]] = None\n",
    "    social_links: Optional[Dict[str, str]] = None\n",
    "    about_page: Optional[str] = None\n",
    "    contact_info: Optional[Dict[str, List[str]]] = None\n",
    "    products_services: Optional[List[Dict[str, str]]] = None\n",
    "    news_data: Optional[List[Dict[str, str]]] = None\n",
    "    pages_scraped: Optional[List[Dict[str, Any]]] = None\n",
    "    total_pages_found: Optional[int] = None\n",
    "\n",
    "class EnhancedTextExtractor:\n",
    "    \"\"\"Enhanced text extraction with comprehensive noise removal\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        logger.debug(\"Initializing EnhancedTextExtractor\")\n",
    "        \n",
    "        # Expanded noise patterns for better cleaning\n",
    "        self.noise_patterns = {\n",
    "            'navigation': [\n",
    "                r'(menu|nav|navigation|breadcrumb|sidebar|footer|header)\\s*:?\\s*',\n",
    "                r'(skip to|jump to|go to)\\s+(content|main|navigation)',\n",
    "                r'(toggle|open|close|show|hide)\\s+(menu|navigation)',\n",
    "                r'(home|back to top|scroll to top)',\n",
    "                r'(previous|next|page \\d+|\\d+ of \\d+)',\n",
    "                r'(sort by|filter by|view all|show more|load more)',\n",
    "                r'(search|submit|reset|cancel|ok|yes|no)\\s*(button)?'\n",
    "            ],\n",
    "            'privacy': [\n",
    "                r'(cookie|privacy|gdpr|ccpa)\\s*(policy|notice|consent|banner)',\n",
    "                r'(accept|decline|manage|customize)\\s*(cookies|privacy|preferences)',\n",
    "                r'(we use cookies|this site uses|by continuing)',\n",
    "                r'(privacy policy|terms of service|legal notice)',\n",
    "                r'(your privacy|data protection|cookie settings)'\n",
    "            ],\n",
    "            'social': [\n",
    "                r'(follow us|connect with us|find us)\\s*on',\n",
    "                r'(share|like|tweet|post)\\s*(this|on|via)',\n",
    "                r'(facebook|twitter|linkedin|instagram|youtube|tiktok)\\s*(page|profile)?',\n",
    "                r'(social media|social networks)',\n",
    "                r'(subscribe|newsletter|email updates)'\n",
    "            ],\n",
    "            'ads': [\n",
    "                r'(advertisement|sponsored|promoted|ad\\s)',\n",
    "                r'(buy now|shop now|order now|get started|sign up)',\n",
    "                r'(free trial|limited time|special offer|discount)',\n",
    "                r'(click here|learn more|read more|find out)',\n",
    "                r'(call now|contact today|get quote)',\n",
    "                r'(\\$\\d+|\\d+% off|save \\$)',\n",
    "                r'(testimonial|review|rating|stars)'\n",
    "            ],\n",
    "            'forms': [\n",
    "                r'(enter|type|select|choose|pick)\\s*(your|a|an)',\n",
    "                r'(required|optional|please|must)\\s*(field|enter|provide)',\n",
    "                r'(email|password|username|phone|address)\\s*(field)?',\n",
    "                r'(checkbox|radio|dropdown|select|input)',\n",
    "                r'(validation|error|success|warning)\\s*(message)?'\n",
    "            ],\n",
    "            'technical': [\n",
    "                r'(loading|please wait|processing)',\n",
    "                r'(javascript|css|browser|compatibility)',\n",
    "                r'(404|error|not found|page not found)',\n",
    "                r'(copyright|all rights reserved|\\(c\\)\\s*\\d{4})',\n",
    "                r'(version|update|upgrade|download)',\n",
    "                r'(api|sdk|documentation|docs)'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Elements to completely remove\n",
    "        self.remove_selectors = [\n",
    "            'nav', 'navbar', '.navbar', '#navbar', '.nav', '#nav',\n",
    "            '.navigation', '#navigation', '.menu', '#menu',\n",
    "            '.breadcrumb', '.breadcrumbs', '.sidebar', '.aside',\n",
    "            'header', 'footer', '.header', '.footer', '#header', '#footer',\n",
    "            '.site-header', '.site-footer', '.page-header', '.page-footer',\n",
    "            '.ad', '.ads', '.advertisement', '.banner', '.promo',\n",
    "            '.marketing', '.cta', '.call-to-action', '.popup', '.modal',\n",
    "            '.overlay', '.lightbox', '[class*=\"ad-\"]', '[id*=\"ad-\"]',\n",
    "            '.social', '.share', '.sharing', '.follow', '.subscribe',\n",
    "            '.newsletter', '.social-media', '.social-links',\n",
    "            '.comments', '.comment', '.review', '.reviews', '.rating',\n",
    "            '.testimonial', '.testimonials', '.feedback',\n",
    "            '.search', '.search-form', '#search', '.login', '.signup',\n",
    "            '.registration', '.newsletter-signup',\n",
    "            'script', 'style', 'noscript', '.hidden', '.invisible',\n",
    "            '[style*=\"display:none\"]', '[style*=\"visibility:hidden\"]',\n",
    "            '.cookie', '.privacy', '.gdpr', '.consent', '.notice',\n",
    "            '.meta', '.metadata', '.tags', '.categories', '.author-info',\n",
    "            '.publish-date', '.last-updated'\n",
    "        ]\n",
    "        \n",
    "        # Content-rich elements to prioritize\n",
    "        self.content_selectors = [\n",
    "            'main', '[role=\"main\"]', '.main', '#main',\n",
    "            '.content', '#content', '.page-content', '.main-content',\n",
    "            'article', '.article', '.post', '.entry',\n",
    "            '.description', '.summary', '.intro', '.overview',\n",
    "            '.about', '.company-info', '.product-info'\n",
    "        ]\n",
    "\n",
    "    def remove_noise_elements(self, soup: BeautifulSoup) -> BeautifulSoup:\n",
    "        \"\"\"Remove noise elements from the soup\"\"\"\n",
    "        logger.debug(\"Removing noise elements from HTML\")\n",
    "        \n",
    "        # Remove comments\n",
    "        comments_removed = 0\n",
    "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "            comment.extract()\n",
    "            comments_removed += 1\n",
    "        logger.debug(f\"Removed {comments_removed} HTML comments\")\n",
    "        \n",
    "        # Remove script, style, meta, link tags\n",
    "        technical_tags_removed = 0\n",
    "        for tag in soup(['script', 'style', 'meta', 'link']):\n",
    "            tag.decompose()\n",
    "            technical_tags_removed += 1\n",
    "        logger.debug(f\"Removed {technical_tags_removed} technical tags\")\n",
    "        \n",
    "        # Remove elements by selector\n",
    "        elements_removed = 0\n",
    "        for selector in self.remove_selectors:\n",
    "            try:\n",
    "                for element in soup.select(selector):\n",
    "                    element.decompose()\n",
    "                    elements_removed += 1\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error removing selector {selector}: {str(e)}\")\n",
    "                continue\n",
    "        logger.debug(f\"Removed {elements_removed} noise elements by selector\")\n",
    "        \n",
    "        # Remove elements with noise-indicating attributes\n",
    "        attr_elements_removed = 0\n",
    "        noise_attributes = [\n",
    "            ('class', ['ad', 'ads', 'advertisement', 'banner', 'popup', 'modal',\n",
    "                      'cookie', 'privacy', 'gdpr', 'social', 'share', 'nav',\n",
    "                      'menu', 'header', 'footer', 'sidebar']),\n",
    "            ('id', ['ad', 'ads', 'banner', 'popup', 'cookie', 'privacy',\n",
    "                   'nav', 'menu', 'header', 'footer']),\n",
    "            ('role', ['banner', 'navigation', 'contentinfo', 'complementary'])\n",
    "        ]\n",
    "        \n",
    "        for attr, keywords in noise_attributes:\n",
    "            try:\n",
    "                elements_to_remove = []\n",
    "                for element in soup.find_all(attrs={attr: True}):\n",
    "                    try:\n",
    "                        if element is None:\n",
    "                            continue\n",
    "                            \n",
    "                        attr_value = element.get(attr, '')\n",
    "                        if attr_value is None:\n",
    "                            continue\n",
    "                            \n",
    "                        if isinstance(attr_value, list):\n",
    "                            attr_value = ' '.join(str(v) for v in attr_value if v is not None)\n",
    "                        \n",
    "                        attr_value = str(attr_value).lower()\n",
    "                        \n",
    "                        if any(keyword in attr_value for keyword in keywords):\n",
    "                            elements_to_remove.append(element)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Error processing individual element for {attr}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                # Remove elements outside the iteration\n",
    "                for element in elements_to_remove:\n",
    "                    try:\n",
    "                        element.decompose()\n",
    "                        attr_elements_removed += 1\n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Error removing element: {str(e)}\")\n",
    "                        continue\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing attribute {attr}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        logger.debug(f\"Removed {attr_elements_removed} elements by noise attributes\")\n",
    "        return soup\n",
    "\n",
    "    def extract_main_content(self, soup: BeautifulSoup) -> str:\n",
    "        \"\"\"Extract main content with priority to content-rich elements\"\"\"\n",
    "        logger.debug(\"Extracting main content from cleaned HTML\")\n",
    "        \n",
    "        content_texts = []\n",
    "        main_content = None\n",
    "        \n",
    "        # Try to find main content area first\n",
    "        for selector in self.content_selectors:\n",
    "            try:\n",
    "                main_content = soup.select_one(selector)\n",
    "                if main_content:\n",
    "                    logger.debug(f\"Found main content using selector: {selector}\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error with content selector {selector}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if main_content:\n",
    "            content_texts.append(self.extract_clean_text(main_content))\n",
    "        else:\n",
    "            logger.debug(\"No main content area found, extracting from body\")\n",
    "            body = soup.find('body')\n",
    "            if body:\n",
    "                content_texts.append(self.extract_clean_text(body))\n",
    "            else:\n",
    "                logger.debug(\"No body found, extracting from entire soup\")\n",
    "                content_texts.append(self.extract_clean_text(soup))\n",
    "        \n",
    "        final_text = '\\n\\n'.join(filter(None, content_texts))\n",
    "        logger.debug(f\"Extracted {len(final_text)} characters of main content\")\n",
    "        return final_text\n",
    "\n",
    "    def extract_clean_text(self, element) -> str:\n",
    "        \"\"\"Extract and clean text from an element\"\"\"\n",
    "        if not element:\n",
    "            return \"\"\n",
    "        \n",
    "        text = element.get_text(separator='\\n', strip=True)\n",
    "        return self.clean_text_content(text)\n",
    "\n",
    "    def clean_text_content(self, text: str) -> str:\n",
    "        \"\"\"Clean text content by removing noise patterns\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        original_length = len(text)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        \n",
    "        # Remove noise patterns\n",
    "        patterns_removed = 0\n",
    "        for category, patterns in self.noise_patterns.items():\n",
    "            for pattern in patterns:\n",
    "                before_length = len(text)\n",
    "                text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "                if len(text) < before_length:\n",
    "                    patterns_removed += 1\n",
    "        \n",
    "        # Remove common UI text patterns\n",
    "        ui_removals = [\n",
    "            r'^(home|menu|navigation|search|close|open)\\s*$',\n",
    "            r'^(skip to|jump to)\\s+\\w+\\s*$',\n",
    "            r'^(page \\d+ of \\d+|previous|next)\\s*$',\n",
    "            r'^(loading|please wait)\\s*\\.{0,3}\\s*$',\n",
    "            r'^(required field|please enter|must be)\\s*\\.{0,3}\\s*$',\n",
    "            r'^[\\*\\+\\-\\•]\\s*$',\n",
    "            r'^\\d+\\s*$',\n",
    "            r'^[^\\w\\s]*$',\n",
    "        ]\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        lines_removed = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            skip_line = False\n",
    "            for pattern in ui_removals:\n",
    "                if re.match(pattern, line, re.IGNORECASE):\n",
    "                    skip_line = True\n",
    "                    lines_removed += 1\n",
    "                    break\n",
    "            \n",
    "            if not skip_line and len(line) > 2:\n",
    "                cleaned_lines.append(line)\n",
    "        \n",
    "        # Join lines and normalize spacing\n",
    "        text = '\\n'.join(cleaned_lines)\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        \n",
    "        final_length = len(text)\n",
    "        logger.debug(f\"Text cleaning: {original_length} -> {final_length} chars, \"\n",
    "                    f\"removed {patterns_removed} noise patterns, {lines_removed} UI lines\")\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "class SmartPageClassifier:\n",
    "    \"\"\"Enhanced page classifier with better patterns\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        logger.debug(\"Initializing SmartPageClassifier\")\n",
    "        \n",
    "        self.page_keywords = {\n",
    "            'about': {\n",
    "                'url_patterns': [\n",
    "                    r'/about', r'/about-us', r'/about-company', r'/our-story', \n",
    "                    r'/who-we-are', r'/mission', r'/vision', r'/values', \n",
    "                    r'/company', r'/overview', r'/history', r'/story',\n",
    "                    r'/why-us', r'/our-mission', r'/our-vision'\n",
    "                ],\n",
    "                'link_text': [\n",
    "                    'about', 'about us', 'about company', 'our story', \n",
    "                    'who we are', 'mission', 'vision', 'company', 'overview',\n",
    "                    'story', 'history', 'background', 'why us', 'our mission'\n",
    "                ],\n",
    "                'content_indicators': [\n",
    "                    'founded', 'established', 'our mission', 'our vision', \n",
    "                    'company history', 'who we are', 'what we do', 'our story',\n",
    "                    'why we exist', 'our purpose'\n",
    "                ]\n",
    "            },\n",
    "            'products': {\n",
    "                'url_patterns': [\n",
    "                    r'/products', r'/product', r'/services', r'/service', r'/solutions', r'/solution',\n",
    "                    r'/offerings', r'/platform', r'/technology', r'/features', r'/what-we-do',\n",
    "                    r'/portfolio', r'/catalog', r'/tools', r'/capabilities', r'/how-it-works',\n",
    "                    r'/pricing', r'/plans'\n",
    "                ],\n",
    "                'link_text': [\n",
    "                    'products', 'product', 'services', 'service', 'solutions', 'solution',\n",
    "                    'offerings', 'platform', 'technology', 'features', 'what we do',\n",
    "                    'portfolio', 'catalog', 'tools', 'capabilities', 'how it works',\n",
    "                    'pricing', 'plans', 'get started'\n",
    "                ],\n",
    "                'content_indicators': [\n",
    "                    'our products', 'our services', 'what we offer', \n",
    "                    'solutions', 'features', 'capabilities', 'platform',\n",
    "                    'how it works', 'pricing', 'plans'\n",
    "                ]\n",
    "            },\n",
    "            'team': {\n",
    "                'url_patterns': [\n",
    "                    r'/team', r'/our-team', r'/leadership', r'/management', \n",
    "                    r'/founders', r'/people', r'/board', r'/executives',\n",
    "                    r'/staff', r'/directors', r'/advisors', r'/meet-the-team',\n",
    "                    r'/our-people', r'/who-we-are'\n",
    "                ],\n",
    "                'link_text': [\n",
    "                    'team', 'our team', 'leadership', 'management', \n",
    "                    'founders', 'people', 'board', 'executives', 'staff',\n",
    "                    'advisors', 'directors', 'meet the team', 'our people',\n",
    "                    'who we are'\n",
    "                ],\n",
    "                'content_indicators': [\n",
    "                    'our team', 'leadership team', 'founders', 'executives',\n",
    "                    'management', 'board of directors', 'meet the team',\n",
    "                    'our people', 'who we are'\n",
    "                ]\n",
    "            },\n",
    "            'contact': {\n",
    "                'url_patterns': [\n",
    "                    r'/contact', r'/contact-us', r'/get-in-touch', r'/reach-us', \n",
    "                    r'/locations', r'/offices', r'/support', r'/help', r'/reach-out',\n",
    "                    r'/talk-to-us', r'/schedule', r'/demo', r'/book'\n",
    "                ],\n",
    "                'link_text': [\n",
    "                    'contact', 'contact us', 'get in touch', 'reach us', \n",
    "                    'locations', 'offices', 'support', 'help', 'reach out',\n",
    "                    'talk to us', 'schedule', 'demo', 'book', 'get started'\n",
    "                ],\n",
    "                'content_indicators': [\n",
    "                    'contact us', 'get in touch', 'phone', 'email', \n",
    "                    'address', 'location', 'office', 'reach out',\n",
    "                    'schedule', 'demo', 'book'\n",
    "                ]\n",
    "            },\n",
    "            'news': {\n",
    "                'url_patterns': [\n",
    "                    r'/news', r'/blog', r'/press', r'/media', r'/updates', \n",
    "                    r'/insights', r'/resources', r'/articles', r'/announcements',\n",
    "                    r'/content', r'/knowledge', r'/learn'\n",
    "                ],\n",
    "                'link_text': [\n",
    "                    'news', 'blog', 'press', 'media', 'updates', \n",
    "                    'insights', 'resources', 'articles', 'announcements',\n",
    "                    'content', 'knowledge', 'learn', 'latest'\n",
    "                ],\n",
    "                'content_indicators': [\n",
    "                    'latest news', 'blog posts', 'press releases', \n",
    "                    'media coverage', 'announcements', 'insights',\n",
    "                    'resources', 'knowledge'\n",
    "                ]\n",
    "            },\n",
    "            'careers': {\n",
    "                'url_patterns': [\n",
    "                    r'/careers', r'/jobs', r'/join-us', r'/work-with-us', \n",
    "                    r'/opportunities', r'/positions', r'/hiring', r'/openings',\n",
    "                    r'/jobs-board', r'/employment'\n",
    "                ],\n",
    "                'link_text': [\n",
    "                    'careers', 'jobs', 'join us', 'work with us', \n",
    "                    'opportunities', 'hiring', 'openings', 'employment',\n",
    "                    'job board', 'work here'\n",
    "                ],\n",
    "                'content_indicators': [\n",
    "                    'join our team', 'career opportunities', 'job openings',\n",
    "                    'work with us', 'we are hiring', 'employment'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def classify_page(self, url: str, link_text: str = \"\", content: str = \"\") -> List[str]:\n",
    "        \"\"\"Classify a page based on URL, link text, and content\"\"\"\n",
    "        classifications = []\n",
    "        url_lower = url.lower()\n",
    "        link_text_lower = link_text.lower()\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        for page_type, keywords in self.page_keywords.items():\n",
    "            score = 0\n",
    "            \n",
    "            # Check URL patterns (highest weight)\n",
    "            for pattern in keywords['url_patterns']:\n",
    "                if re.search(pattern, url_lower):\n",
    "                    score += 4\n",
    "                    break\n",
    "            \n",
    "            # Check link text (medium weight)\n",
    "            for text in keywords['link_text']:\n",
    "                if text in link_text_lower:\n",
    "                    score += 2\n",
    "                    break\n",
    "            \n",
    "            # Check content indicators (lower weight)\n",
    "            if content:\n",
    "                for indicator in keywords['content_indicators']:\n",
    "                    if indicator in content_lower:\n",
    "                        score += 1\n",
    "                        break\n",
    "            \n",
    "            if score >= 2:\n",
    "                classifications.append(page_type)\n",
    "        \n",
    "        logger.debug(f\"Classified URL {url} as: {classifications}\")\n",
    "        return classifications\n",
    "\n",
    "def normalize_url(url: str, base_url: str) -> str:\n",
    "    \"\"\"Normalize and resolve URLs with better deduplication\"\"\"\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    \n",
    "    url = url.split('#')[0].strip()\n",
    "    \n",
    "    if url.startswith('/'):\n",
    "        normalized = urljoin(base_url, url)\n",
    "    elif not url.startswith(('http://', 'https://')):\n",
    "        normalized = urljoin(base_url, url)\n",
    "    else:\n",
    "        normalized = url\n",
    "    \n",
    "    # More aggressive trailing slash normalization\n",
    "    # Always remove trailing slash except for root domain\n",
    "    parsed = urlparse(normalized)\n",
    "    if parsed.path and parsed.path != '/' and parsed.path.endswith('/'):\n",
    "        normalized = normalized.rstrip('/')\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def urls_are_equivalent(url1: str, url2: str) -> bool:\n",
    "    \"\"\"Check if two URLs are equivalent (same content)\"\"\"\n",
    "    # Normalize both URLs\n",
    "    parsed1 = urlparse(url1.rstrip('/'))\n",
    "    parsed2 = urlparse(url2.rstrip('/'))\n",
    "    \n",
    "    # Same if domain and path are the same\n",
    "    return (parsed1.netloc == parsed2.netloc and \n",
    "            parsed1.path.rstrip('/') == parsed2.path.rstrip('/'))\n",
    "\n",
    "def deduplicate_urls(urls_with_context: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Remove duplicate URLs that point to the same content\"\"\"\n",
    "    seen_urls = set()\n",
    "    deduplicated = []\n",
    "    \n",
    "    for url, context in urls_with_context:\n",
    "        # Create a canonical version for comparison\n",
    "        parsed = urlparse(url.rstrip('/'))\n",
    "        canonical = f\"{parsed.netloc}{parsed.path.rstrip('/')}\"\n",
    "        \n",
    "        if canonical not in seen_urls:\n",
    "            seen_urls.add(canonical)\n",
    "            # Use the cleaner URL (without trailing slash)\n",
    "            clean_url = f\"{parsed.scheme}://{parsed.netloc}{parsed.path.rstrip('/')}\"\n",
    "            if clean_url.endswith('://'):  # Handle edge case for root domain\n",
    "                clean_url += parsed.netloc\n",
    "            deduplicated.append((clean_url, context))\n",
    "        else:\n",
    "            logger.debug(f\"Skipping duplicate URL: {url}\")\n",
    "    \n",
    "    return deduplicated\n",
    "\n",
    "def is_same_domain(url1: str, url2: str) -> bool:\n",
    "    \"\"\"Check if two URLs are from the same domain\"\"\"\n",
    "    return urlparse(url1).netloc == urlparse(url2).netloc\n",
    "\n",
    "def should_skip_url(url: str) -> bool:\n",
    "    \"\"\"Determine if a URL should be skipped\"\"\"\n",
    "    url_lower = url.lower()\n",
    "    \n",
    "    skip_extensions = ('.pdf', '.doc', '.docx', '.xls', '.xlsx', '.zip', \n",
    "                      '.png', '.jpg', '.jpeg', '.gif', '.svg', '.ico',\n",
    "                      '.css', '.js', '.xml', '.json', '.rss', '.txt')\n",
    "    \n",
    "    skip_paths = ('/login', '/signin', '/signup', '/register', '/cart', \n",
    "                 '/checkout', '/privacy', '/terms', '/cookie', '/legal',\n",
    "                 '/wp-admin', '/admin', '/dashboard', '/account', '/user',\n",
    "                 '/auth', '/oauth', '/api', '/download')\n",
    "    \n",
    "    skip_domains = ('facebook.com', 'twitter.com', 'linkedin.com', \n",
    "                   'instagram.com', 'youtube.com', 'google.com',\n",
    "                   'github.com', 'medium.com', 'reddit.com')\n",
    "    \n",
    "    should_skip = (any(ext in url_lower for ext in skip_extensions) or\n",
    "                   any(path in url_lower for path in skip_paths) or\n",
    "                   any(domain in url_lower for domain in skip_domains))\n",
    "    \n",
    "    if should_skip:\n",
    "        logger.debug(f\"Skipping URL: {url}\")\n",
    "    \n",
    "    return should_skip\n",
    "\n",
    "def extract_links_with_context(soup: BeautifulSoup, base_url: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Extract links with their anchor text context - enhanced for modern websites\"\"\"\n",
    "    logger.debug(\"Extracting links with context\")\n",
    "    \n",
    "    links_with_context = []\n",
    "    seen_urls = set()\n",
    "    \n",
    "    # Method 1: Traditional <a> tags\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link.get('href', '').strip()\n",
    "        if not href or href.startswith(('mailto:', 'tel:', 'javascript:')):\n",
    "            continue\n",
    "        \n",
    "        normalized_url = normalize_url(href, base_url)\n",
    "        if not normalized_url or should_skip_url(normalized_url):\n",
    "            continue\n",
    "        \n",
    "        if normalized_url in seen_urls:\n",
    "            continue\n",
    "        \n",
    "        base_domain = base_url.rstrip('/')\n",
    "        if normalized_url in [base_domain, base_domain + '/', base_domain + '/index.html', base_domain + '/home']:\n",
    "            continue\n",
    "        \n",
    "        seen_urls.add(normalized_url)\n",
    "        \n",
    "        link_text = link.get_text(strip=True)\n",
    "        if not link_text or len(link_text) < 2 or link_text.lower() in ['home', 'here', 'click', 'more', '»', '›']:\n",
    "            continue\n",
    "        \n",
    "        parent_text = \"\"\n",
    "        parent = link.parent\n",
    "        while parent and not parent_text and parent.name != 'body':\n",
    "            parent_text = parent.get_text(strip=True)[:100]\n",
    "            parent = parent.parent\n",
    "        \n",
    "        context = f\"{link_text} {parent_text}\".strip()\n",
    "        links_with_context.append((normalized_url, context))\n",
    "    \n",
    "    # Method 2: Look for data attributes that might contain URLs (for SPAs)\n",
    "    for element in soup.find_all(attrs={\"data-href\": True}):\n",
    "        href = element.get('data-href', '').strip()\n",
    "        if href:\n",
    "            normalized_url = normalize_url(href, base_url)\n",
    "            if normalized_url and not should_skip_url(normalized_url) and normalized_url not in seen_urls:\n",
    "                seen_urls.add(normalized_url)\n",
    "                text = element.get_text(strip=True)\n",
    "                links_with_context.append((normalized_url, text))\n",
    "    \n",
    "    # Method 3: Common navigation patterns and text-based discovery\n",
    "    common_pages = [\n",
    "        ('about', ['about', 'about-us', 'about-company', 'our-story', 'company', 'who-we-are']),\n",
    "        ('products', ['products', 'product', 'solutions', 'services', 'platform', 'features']),\n",
    "        ('team', ['team', 'our-team', 'leadership', 'founders', 'people', 'management']),\n",
    "        ('contact', ['contact', 'contact-us', 'get-in-touch', 'reach-us', 'support']),\n",
    "        ('news', ['news', 'blog', 'press', 'media', 'updates', 'insights', 'resources']),\n",
    "        ('careers', ['careers', 'jobs', 'join-us', 'work-with-us', 'opportunities'])\n",
    "    ]\n",
    "    \n",
    "    # Method 4: Look for text that suggests navigation but might not be linked properly\n",
    "    all_text = soup.get_text().lower()\n",
    "    parsed_url = urlparse(base_url)\n",
    "    domain = parsed_url.netloc\n",
    "    \n",
    "    for page_type, variations in common_pages:\n",
    "        for variation in variations:\n",
    "            # Check if the text appears on the page (suggesting the section exists)\n",
    "            if variation in all_text:\n",
    "                # Try common URL patterns\n",
    "                potential_urls = [\n",
    "                    f\"{base_url}/{variation}\",\n",
    "                    f\"{base_url}/{variation}/\",\n",
    "                    f\"https://{domain}/{variation}\",\n",
    "                    f\"https://www.{domain}/{variation}\" if not domain.startswith('www.') else f\"https://{domain[4:]}/{variation}\"\n",
    "                ]\n",
    "                \n",
    "                for potential_url in potential_urls:\n",
    "                    if potential_url not in seen_urls and not should_skip_url(potential_url):\n",
    "                        seen_urls.add(potential_url)\n",
    "                        links_with_context.append((potential_url, f\"{variation} (discovered from content)\"))\n",
    "                        logger.debug(f\"Discovered potential page: {potential_url}\")\n",
    "                        break  # Only add one variation per page type\n",
    "    \n",
    "    # Method 5: Look for navigation menus and buttons that might not be proper links\n",
    "    nav_selectors = [\n",
    "        'nav', '.nav', '.navigation', '.navbar', '.menu', \n",
    "        '[role=\"navigation\"]', '.header-nav', '.main-nav'\n",
    "    ]\n",
    "    \n",
    "    for selector in nav_selectors:\n",
    "        try:\n",
    "            nav_elements = soup.select(selector)\n",
    "            for nav in nav_elements:\n",
    "                # Look for clickable elements within navigation\n",
    "                for element in nav.find_all(['button', 'div', 'span', 'li']):\n",
    "                    text = element.get_text(strip=True).lower()\n",
    "                    if text and len(text) > 2:\n",
    "                        for page_type, variations in common_pages:\n",
    "                            if text in variations:\n",
    "                                potential_url = f\"{base_url}/{text}\"\n",
    "                                if potential_url not in seen_urls and not should_skip_url(potential_url):\n",
    "                                    seen_urls.add(potential_url)\n",
    "                                    links_with_context.append((potential_url, f\"{text} (from navigation)\"))\n",
    "                                    logger.debug(f\"Discovered nav page: {potential_url}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error processing nav selector {selector}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    logger.debug(f\"Found {len(links_with_context)} valid internal links\")\n",
    "    return links_with_context\n",
    "\n",
    "def extract_contact_info(soup, text):\n",
    "    \"\"\"Extract contact information from the page\"\"\"\n",
    "    logger.debug(\"Extracting contact information\")\n",
    "    contact_info = {}\n",
    "    \n",
    "    try:\n",
    "        # Email extraction\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        emails = re.findall(email_pattern, text)\n",
    "        if emails:\n",
    "            valid_emails = [email for email in emails if not any(skip in email.lower() \n",
    "                           for skip in ['@example.com', '@domain.com', '@yoursite.com', '@test.com'])]\n",
    "            if valid_emails:\n",
    "                contact_info['emails'] = list(set(valid_emails))[:5]\n",
    "                logger.debug(f\"Found {len(contact_info['emails'])} valid emails\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting emails: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        # Phone extraction\n",
    "        phone_patterns = [\n",
    "            r'(\\+?1?[-.\\s]?\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4})',\n",
    "            r'(\\+?[0-9]{1,3}[-.\\s]?[0-9]{1,4}[-.\\s]?[0-9]{1,4}[-.\\s]?[0-9]{1,9})'\n",
    "        ]\n",
    "        \n",
    "        phones = []\n",
    "        for pattern in phone_patterns:\n",
    "            phones.extend(re.findall(pattern, text))\n",
    "        \n",
    "        if phones:\n",
    "            contact_info['phones'] = list(set(phones))[:3]\n",
    "            logger.debug(f\"Found {len(contact_info['phones'])} phone numbers\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting phones: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        # Address extraction\n",
    "        address_patterns = [\n",
    "            r'(?i)address[:\\s]+([^.]*(?:street|st|avenue|ave|road|rd|drive|dr|boulevard|blvd|lane|ln)[^.]*)',\n",
    "            r'([0-9]+\\s+[A-Za-z\\s]+(?:street|st|avenue|ave|road|rd|drive|dr|boulevard|blvd|lane|ln)[^.]*)'\n",
    "        ]\n",
    "        \n",
    "        addresses = []\n",
    "        for pattern in address_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            addresses.extend(matches)\n",
    "        \n",
    "        if addresses:\n",
    "            contact_info['addresses'] = list(set(addresses))[:3]\n",
    "            logger.debug(f\"Found {len(contact_info['addresses'])} addresses\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting addresses: {str(e)}\")\n",
    "    \n",
    "    return contact_info\n",
    "\n",
    "def extract_team_info(soup):\n",
    "    \"\"\"Extract team information from the page\"\"\"\n",
    "    logger.debug(\"Extracting team information\")\n",
    "    team_info = []\n",
    "    \n",
    "    try:\n",
    "        team_selectors = [\n",
    "            {'class': re.compile(r'team|member|founder|leadership|staff|employee', re.I)},\n",
    "            {'id': re.compile(r'team|member|founder|leadership|staff', re.I)}\n",
    "        ]\n",
    "        \n",
    "        team_sections = []\n",
    "        for selector in team_selectors:\n",
    "            try:\n",
    "                team_sections.extend(soup.find_all(['div', 'section', 'article'], selector))\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error with team selector: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        logger.debug(f\"Found {len(team_sections)} potential team sections\")\n",
    "        \n",
    "        for section in team_sections:\n",
    "            try:\n",
    "                names = section.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'strong', 'b'])\n",
    "                for name_elem in names:\n",
    "                    name_text = name_elem.get_text().strip()\n",
    "                    \n",
    "                    if (2 <= len(name_text.split()) <= 4 and \n",
    "                        len(name_text) < 50 and \n",
    "                        not any(skip in name_text.lower() for skip in ['team', 'leadership', 'about', 'contact'])):\n",
    "                        \n",
    "                        title = \"\"\n",
    "                        \n",
    "                        for sibling in name_elem.find_next_siblings(['p', 'div', 'span', 'h6'])[:3]:\n",
    "                            sibling_text = sibling.get_text().strip()\n",
    "                            if sibling_text and len(sibling_text) < 100:\n",
    "                                title = sibling_text\n",
    "                                break\n",
    "                        \n",
    "                        if not title:\n",
    "                            parent = name_elem.parent\n",
    "                            if parent:\n",
    "                                title_elem = parent.find(['p', 'span', 'div'])\n",
    "                                if title_elem:\n",
    "                                    title = title_elem.get_text().strip()[:100]\n",
    "                        \n",
    "                        team_info.append({\n",
    "                            'name': name_text,\n",
    "                            'title': title\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing team section: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen_names = set()\n",
    "        unique_team_info = []\n",
    "        for member in team_info:\n",
    "            if member['name'] not in seen_names:\n",
    "                seen_names.add(member['name'])\n",
    "                unique_team_info.append(member)\n",
    "        \n",
    "        logger.debug(f\"Extracted {len(unique_team_info)} unique team members\")\n",
    "        return unique_team_info[:10]\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting team info: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_products_services(soup):\n",
    "    \"\"\"Extract products and services information\"\"\"\n",
    "    logger.debug(\"Extracting products and services\")\n",
    "    products_services = []\n",
    "    \n",
    "    try:\n",
    "        product_selectors = [\n",
    "            {'class': re.compile(r'product|service|offering|solution|feature', re.I)},\n",
    "            {'id': re.compile(r'product|service|offering|solution', re.I)}\n",
    "        ]\n",
    "        \n",
    "        sections = []\n",
    "        for selector in product_selectors:\n",
    "            try:\n",
    "                sections.extend(soup.find_all(['div', 'section', 'article'], selector))\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error with product selector: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        logger.debug(f\"Found {len(sections)} potential product/service sections\")\n",
    "        \n",
    "        for section in sections:\n",
    "            try:\n",
    "                title_elem = section.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "                if title_elem:\n",
    "                    title = title_elem.get_text().strip()\n",
    "                    \n",
    "                    description = \"\"\n",
    "                    desc_elem = section.find(['p', 'div'])\n",
    "                    if desc_elem:\n",
    "                        description = desc_elem.get_text().strip()\n",
    "                    \n",
    "                    if title and len(title) > 3:\n",
    "                        product_info = {\n",
    "                            'title': title[:200],\n",
    "                            'description': description[:1000]\n",
    "                        }\n",
    "                        products_services.append(product_info)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing product section: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        logger.debug(f\"Extracted {len(products_services)} products/services\")\n",
    "        return products_services[:10]\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting products/services: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_news_posts(soup):\n",
    "    \"\"\"Extract news/blog posts\"\"\"\n",
    "    logger.debug(\"Extracting news/blog posts\")\n",
    "    news_data = []\n",
    "    \n",
    "    try:\n",
    "        news_selectors = [\n",
    "            {'class': re.compile(r'news|blog|post|article|press|update', re.I)},\n",
    "            {'id': re.compile(r'news|blog|post|article|press', re.I)}\n",
    "        ]\n",
    "        \n",
    "        sections = []\n",
    "        for selector in news_selectors:\n",
    "            try:\n",
    "                sections.extend(soup.find_all(['div', 'article', 'section'], selector))\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error with news selector: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        logger.debug(f\"Found {len(sections)} potential news sections\")\n",
    "        \n",
    "        for section in sections:\n",
    "            try:\n",
    "                title_elem = section.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "                if title_elem:\n",
    "                    title = title_elem.get_text().strip()\n",
    "                    \n",
    "                    date = \"\"\n",
    "                    date_patterns = [\n",
    "                        {'class': re.compile(r'date|time|published', re.I)},\n",
    "                        {'datetime': True}\n",
    "                    ]\n",
    "                    \n",
    "                    for pattern in date_patterns:\n",
    "                        try:\n",
    "                            date_elem = section.find(['time', 'span', 'div'], pattern)\n",
    "                            if date_elem:\n",
    "                                date = date_elem.get_text().strip()\n",
    "                                break\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                    \n",
    "                    content = \"\"\n",
    "                    try:\n",
    "                        content_elem = section.find(['p', 'div'])\n",
    "                        if content_elem:\n",
    "                            content = content_elem.get_text().strip()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    \n",
    "                    if title and len(title) > 5:\n",
    "                        news_data.append({\n",
    "                            'title': title[:300],\n",
    "                            'date': date[:50],\n",
    "                            'content': content[:800]\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing news section: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        logger.debug(f\"Extracted {len(news_data)} news items\")\n",
    "        return news_data[:10]\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting news posts: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_company_name(soup: BeautifulSoup, url: str) -> str:\n",
    "    \"\"\"Extract company name with better fallbacks and logging\"\"\"\n",
    "    logger.debug(\"Extracting company name\")\n",
    "    \n",
    "    marketing_keywords = [\n",
    "        'get', 'start', 'try', 'buy', 'learn', 'discover', 'create', 'build', \n",
    "        'best', 'top', 'leading', 'ultimate', 'perfect', 'easy', 'simple',\n",
    "        'that', 'which', 'for your', 'solution', 'platform', 'software'\n",
    "    ]\n",
    "    \n",
    "    def looks_like_marketing(text: str) -> bool:\n",
    "        if not text or len(text) > 60:\n",
    "            return True\n",
    "        text_lower = text.lower()\n",
    "        marketing_count = sum(1 for keyword in marketing_keywords if keyword in text_lower)\n",
    "        return marketing_count >= 2\n",
    "    \n",
    "    def extract_from_domain(url: str) -> str:\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "        domain_name = domain.split('.')[0]\n",
    "        company_name = domain_name.replace('-', ' ').replace('_', ' ')\n",
    "        return ' '.join(word.capitalize() for word in company_name.split())\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    # Try multiple sources\n",
    "    try:\n",
    "        if soup.title:\n",
    "            title = soup.title.string.strip()\n",
    "            logger.debug(f\"Found page title: {title}\")\n",
    "            \n",
    "            for separator in ['|', '-', '–', ':', '•']:\n",
    "                if separator in title:\n",
    "                    parts = [part.strip() for part in title.split(separator)]\n",
    "                    for part in parts:\n",
    "                        if part and not looks_like_marketing(part):\n",
    "                            candidates.append(part)\n",
    "                            logger.debug(f\"Added title candidate: {part}\")\n",
    "                    break\n",
    "            if not candidates and not looks_like_marketing(title):\n",
    "                candidates.append(title)\n",
    "                logger.debug(f\"Added full title as candidate: {title}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting from title: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        og_site_name = soup.find('meta', property='og:site_name')\n",
    "        if og_site_name and og_site_name.get('content'):\n",
    "            site_name = og_site_name['content'].strip()\n",
    "            logger.debug(f\"Found OG site name: {site_name}\")\n",
    "            if not looks_like_marketing(site_name):\n",
    "                candidates.append(site_name)\n",
    "                logger.debug(f\"Added OG site name as candidate: {site_name}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting OG site name: {str(e)}\")\n",
    "    \n",
    "    final_name = candidates[0] if candidates else extract_from_domain(url)\n",
    "    logger.info(f\"Extracted company name: {final_name}\")\n",
    "    return final_name\n",
    "\n",
    "def extract_description(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"Extract site description with logging\"\"\"\n",
    "    logger.debug(\"Extracting site description\")\n",
    "    \n",
    "    try:\n",
    "        meta_desc = soup.find('meta', {'name': 'description'})\n",
    "        if meta_desc and meta_desc.get('content'):\n",
    "            desc = meta_desc['content'].strip()\n",
    "            logger.debug(f\"Found meta description: {desc[:100]}...\")\n",
    "            return desc\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting meta description: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        og_desc = soup.find('meta', property='og:description')\n",
    "        if og_desc and og_desc.get('content'):\n",
    "            desc = og_desc['content'].strip()\n",
    "            logger.debug(f\"Found OG description: {desc[:100]}...\")\n",
    "            return desc\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting OG description: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        first_p = soup.find('p')\n",
    "        if first_p:\n",
    "            text = first_p.get_text().strip()\n",
    "            if len(text) > 50:\n",
    "                desc = text[:300]\n",
    "                logger.debug(f\"Found first paragraph description: {desc[:100]}...\")\n",
    "                return desc\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting first paragraph: {str(e)}\")\n",
    "    \n",
    "    logger.debug(\"No description found\")\n",
    "    return \"\"\n",
    "\n",
    "def extract_social_links(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"Extract social media links with logging\"\"\"\n",
    "    logger.debug(\"Extracting social media links\")\n",
    "    social_links = {}\n",
    "    \n",
    "    try:\n",
    "        social_platforms = {\n",
    "            'twitter': ['twitter.com', 'x.com'],\n",
    "            'linkedin': ['linkedin.com'],\n",
    "            'facebook': ['facebook.com'],\n",
    "            'instagram': ['instagram.com'],\n",
    "            'github': ['github.com'],\n",
    "            'youtube': ['youtube.com']\n",
    "        }\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            try:\n",
    "                href = link['href'].lower()\n",
    "                for platform, domains in social_platforms.items():\n",
    "                    if any(domain in href for domain in domains):\n",
    "                        if href.startswith('//'):\n",
    "                            href = 'https:' + href\n",
    "                        elif not href.startswith('http'):\n",
    "                            continue\n",
    "                        social_links[platform] = href\n",
    "                        logger.debug(f\"Found {platform} link: {href}\")\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing social link: {str(e)}\")\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error extracting social links: {str(e)}\")\n",
    "    \n",
    "    logger.debug(f\"Found {len(social_links)} social media links\")\n",
    "    return social_links\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text content with logging\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    original_length = len(text)\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove common navigation and UI text\n",
    "    ui_patterns = [\n",
    "        r'(menu|navigation|search|close|open|toggle|submit|cancel|accept cookies)\\b',\n",
    "        r'(follow us on|share on|like us on|connect with us on)\\b.*',\n",
    "        r'(skip to content|skip navigation|accessibility)',\n",
    "        r'(cookie policy|privacy policy|terms of service)\\b'\n",
    "    ]\n",
    "    \n",
    "    for pattern in ui_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    final_length = len(text)\n",
    "    if original_length != final_length:\n",
    "        logger.debug(f\"Text cleaning: {original_length} -> {final_length} characters\")\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "async def scrape_website(url: str, max_pages: int = 5) -> ScrapedData:\n",
    "    \"\"\"\n",
    "    Enhanced website scraping with smart page discovery and classification.\n",
    "    Continues scraping even if individual sections encounter errors.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the startup's website to scrape\n",
    "        max_pages (int, optional): Maximum number of pages to scrape. Defaults to 5.\n",
    "        \n",
    "    Returns:\n",
    "        ScrapedData: Structured data extracted from the website\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting enhanced scraping of {url} with max_pages: {max_pages}\")\n",
    "    \n",
    "    text_extractor = EnhancedTextExtractor()\n",
    "    classifier = SmartPageClassifier()\n",
    "    pages_scraped = []\n",
    "    all_discovered_links = set()\n",
    "    classified_pages = defaultdict(list)\n",
    "    pages_to_scrape = deque([(url, \"\")])\n",
    "    \n",
    "    # Initialize session with better configuration\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "    })\n",
    "    \n",
    "    retries = requests.adapters.Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[500, 502, 503, 504, 429],\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    session.mount('http://', requests.adapters.HTTPAdapter(max_retries=retries))\n",
    "    session.mount('https://', requests.adapters.HTTPAdapter(max_retries=retries))\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Scrape main page\n",
    "        logger.info(\"Step 1: Scraping main page...\")\n",
    "        try:\n",
    "            logger.debug(f\"Fetching URL: {url}\")\n",
    "            response = session.get(url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            logger.debug(f\"Successfully fetched main page, status: {response.status_code}\")\n",
    "            \n",
    "            main_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            logger.debug(\"Successfully parsed HTML with BeautifulSoup\")\n",
    "            \n",
    "            # Remove noise elements and extract clean text\n",
    "            cleaned_soup = text_extractor.remove_noise_elements(main_soup)\n",
    "            main_text = text_extractor.extract_main_content(cleaned_soup)\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            error_msg = f\"Failed to fetch website {url}: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            return ScrapedData(\n",
    "                company_name=urlparse(url).netloc,\n",
    "                description=f\"Error fetching website: {str(e)}\",\n",
    "                raw_text=\"\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error parsing website {url}: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            return ScrapedData(\n",
    "                company_name=urlparse(url).netloc,\n",
    "                description=f\"Error parsing website: {str(e)}\",\n",
    "                raw_text=\"\"\n",
    "            )\n",
    "        \n",
    "        # Extract basic company info from main page\n",
    "        logger.info(\"Step 2: Extracting basic company information...\")\n",
    "        try:\n",
    "            company_name = extract_company_name(main_soup, url)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error extracting company name: {str(e)}\")\n",
    "            company_name = urlparse(url).netloc\n",
    "        \n",
    "        try:\n",
    "            description = extract_description(main_soup)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error extracting description: {str(e)}\")\n",
    "            description = \"\"\n",
    "        \n",
    "        try:\n",
    "            social_links = extract_social_links(main_soup)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error extracting social links: {str(e)}\")\n",
    "            social_links = {}\n",
    "        \n",
    "        # Initialize combined text with main page\n",
    "        all_text = f\"--- MAIN PAGE ---\\n{main_text}\"\n",
    "        logger.debug(f\"Main page text length: {len(main_text)} characters\")\n",
    "        \n",
    "        pages_scraped.append({\n",
    "            'url': url,\n",
    "            'type': 'main',\n",
    "            'title': main_soup.title.string if main_soup.title else 'Main Page',\n",
    "            'word_count': len(main_text.split())\n",
    "        })\n",
    "        \n",
    "        # Step 3: Discover all internal links from main page\n",
    "        logger.info(\"Step 3: Discovering internal links...\")\n",
    "        base_url = f\"{urlparse(url).scheme}://{urlparse(url).netloc}\"\n",
    "        links_with_context = extract_links_with_context(main_soup, base_url)\n",
    "        \n",
    "        # Deduplicate URLs before processing\n",
    "        links_with_context = deduplicate_urls(links_with_context)\n",
    "        logger.debug(f\"After deduplication: {len(links_with_context)} unique links\")\n",
    "        \n",
    "        # Step 4: Classify discovered links and verify they exist\n",
    "        logger.info(\"Step 4: Classifying and verifying discovered links...\")\n",
    "        unique_links = {}\n",
    "        verified_links = []\n",
    "        scraped_urls = set()  # Track URLs we've already scraped\n",
    "        \n",
    "        for link_url, context in links_with_context:\n",
    "            if is_same_domain(link_url, url) and link_url != url:\n",
    "                if link_url not in unique_links:\n",
    "                    unique_links[link_url] = context\n",
    "                    \n",
    "                    # Verify the URL exists before adding to discovered links\n",
    "                    try:\n",
    "                        # Quick HEAD request to check if URL exists\n",
    "                        head_response = session.head(link_url, timeout=5, allow_redirects=True)\n",
    "                        if head_response.status_code < 400:\n",
    "                            all_discovered_links.add((link_url, context))\n",
    "                            verified_links.append((link_url, context))\n",
    "                            logger.debug(f\"Verified URL exists: {link_url}\")\n",
    "                        else:\n",
    "                            logger.debug(f\"URL returned {head_response.status_code}: {link_url}\")\n",
    "                    except Exception as e:\n",
    "                        # If HEAD fails, try GET as some servers don't support HEAD\n",
    "                        try:\n",
    "                            get_response = session.get(link_url, timeout=5, allow_redirects=True)\n",
    "                            if get_response.status_code < 400:\n",
    "                                all_discovered_links.add((link_url, context))\n",
    "                                verified_links.append((link_url, context))\n",
    "                                logger.debug(f\"Verified URL exists (GET): {link_url}\")\n",
    "                            else:\n",
    "                                logger.debug(f\"URL not accessible: {link_url} - {str(e)}\")\n",
    "                        except Exception as e2:\n",
    "                            logger.debug(f\"URL verification failed: {link_url} - {str(e2)}\")\n",
    "                            continue\n",
    "        \n",
    "        # Classify verified links with deduplication\n",
    "        for link_url, context in verified_links:\n",
    "            classifications = classifier.classify_page(link_url, context)\n",
    "            for classification in classifications:\n",
    "                # Only add if we haven't already classified this URL for this type\n",
    "                existing_urls = [existing_url for existing_url, _ in classified_pages[classification]]\n",
    "                if not any(urls_are_equivalent(link_url, existing_url) for existing_url in existing_urls):\n",
    "                    classified_pages[classification].append((link_url, context))\n",
    "        \n",
    "        logger.info(f\"Discovered {len(all_discovered_links)} verified internal links\")\n",
    "        logger.info(f\"Classified pages: {dict(classified_pages)}\")\n",
    "        \n",
    "        # If we didn't find many pages, try some additional discovery methods\n",
    "        if len(all_discovered_links) < 3:\n",
    "            logger.info(\"Few pages discovered, trying additional discovery methods...\")\n",
    "            \n",
    "            # Method: Check common page patterns directly\n",
    "            common_paths = [\n",
    "                'about', 'about-us', 'company', 'our-story', 'who-we-are',\n",
    "                'products', 'product', 'services', 'solutions', 'platform', 'features',\n",
    "                'team', 'our-team', 'leadership', 'founders', 'people',\n",
    "                'contact', 'contact-us', 'get-in-touch', 'support',\n",
    "                'news', 'blog', 'press', 'media', 'updates', 'insights'\n",
    "            ]\n",
    "            \n",
    "            additional_found = 0\n",
    "            for path in common_paths:\n",
    "                test_url = f\"{base_url}/{path}\"\n",
    "                \n",
    "                # Check if we already have an equivalent URL\n",
    "                if any(urls_are_equivalent(test_url, existing_url) for existing_url, _ in all_discovered_links):\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    test_response = session.head(test_url, timeout=3, allow_redirects=True)\n",
    "                    if test_response.status_code < 400:\n",
    "                        all_discovered_links.add((test_url, f\"{path} (direct discovery)\"))\n",
    "                        classifications = classifier.classify_page(test_url, path)\n",
    "                        for classification in classifications:\n",
    "                            # Check for equivalent URLs in this classification\n",
    "                            existing_urls = [existing_url for existing_url, _ in classified_pages[classification]]\n",
    "                            if not any(urls_are_equivalent(test_url, existing_url) for existing_url in existing_urls):\n",
    "                                classified_pages[classification].append((test_url, f\"{path} (direct discovery)\"))\n",
    "                        additional_found += 1\n",
    "                        logger.debug(f\"Direct discovery found: {test_url}\")\n",
    "                        \n",
    "                        if additional_found >= 5:  # Limit additional discoveries\n",
    "                            break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            if additional_found > 0:\n",
    "                logger.info(f\"Additional discovery found {additional_found} more pages\")\n",
    "        \n",
    "        \n",
    "        # Step 5: Prioritize pages to scrape based on classification and max_pages\n",
    "        logger.info(\"Step 5: Prioritizing pages to scrape...\")\n",
    "        priority_pages = []\n",
    "        \n",
    "        # Define key page types in order of importance\n",
    "        key_page_types = ['about', 'team', 'products', 'contact', 'news']\n",
    "        \n",
    "        # First, add one page from each key type if available\n",
    "        for page_type in key_page_types:\n",
    "            if page_type in classified_pages and classified_pages[page_type]:\n",
    "                link_url, context = classified_pages[page_type][0]\n",
    "                if len(priority_pages) < max_pages - 1:  # -1 for main page\n",
    "                    priority_pages.append((link_url, context, page_type))\n",
    "                    logger.debug(f\"Added {page_type} page to priority: {link_url}\")\n",
    "        \n",
    "        # Then fill remaining slots with other classified pages\n",
    "        remaining_slots = max_pages - len(priority_pages) - 1  # -1 for main page\n",
    "        if remaining_slots > 0:\n",
    "            # Get remaining classified pages\n",
    "            remaining_classified = [\n",
    "                (url, ctx, ptype)\n",
    "                for ptype in classified_pages\n",
    "                for url, ctx in classified_pages[ptype]\n",
    "                if not any(url == p[0] for p in priority_pages)\n",
    "            ]\n",
    "            \n",
    "            # Sort by URL structure (shorter URLs often more important)\n",
    "            remaining_classified.sort(key=lambda x: (len(x[0].split('/')), x[0]))\n",
    "            \n",
    "            # Add remaining classified pages\n",
    "            priority_pages.extend(remaining_classified[:remaining_slots])\n",
    "            logger.debug(f\"Added {len(remaining_classified[:remaining_slots])} additional pages\")\n",
    "        \n",
    "        # Step 6: Scrape prioritized pages with deduplication\n",
    "        logger.info(f\"Step 6: Scraping {len(priority_pages)} prioritized pages...\")\n",
    "        scraped_data_parts = {\n",
    "            'about_page': None,\n",
    "            'contact_info': {},\n",
    "            'team_info': [],\n",
    "            'products_services': [],\n",
    "            'news_data': []\n",
    "        }\n",
    "        \n",
    "        scraped_urls = set()  # Track URLs we've actually scraped content from\n",
    "        \n",
    "        for i, (page_url, context, page_type) in enumerate(priority_pages, 1):\n",
    "            try:\n",
    "                # Skip if we've already scraped an equivalent URL\n",
    "                if any(urls_are_equivalent(page_url, scraped_url) for scraped_url in scraped_urls):\n",
    "                    logger.debug(f\"Skipping duplicate URL: {page_url}\")\n",
    "                    continue\n",
    "                \n",
    "                if page_url == url:  # Skip if same as main page\n",
    "                    continue\n",
    "                \n",
    "                logger.info(f\"Scraping page {i}/{len(priority_pages)}: {page_url} ({page_type})\")\n",
    "                \n",
    "                # Add delay to be respectful\n",
    "                if i > 0:\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                page_response = session.get(page_url, timeout=15)\n",
    "                page_response.raise_for_status()\n",
    "                logger.debug(f\"Successfully fetched {page_url}, status: {page_response.status_code}\")\n",
    "                \n",
    "                # Verify content type\n",
    "                content_type = page_response.headers.get('content-type', '')\n",
    "                if not any(ct in content_type.lower() for ct in ['text/html', 'application/xhtml']):\n",
    "                    logger.warning(f\"Skipping {page_url}: Invalid content type {content_type}\")\n",
    "                    continue\n",
    "                \n",
    "                page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
    "                \n",
    "                # Clean and extract text\n",
    "                try:\n",
    "                    cleaned_page_soup = text_extractor.remove_noise_elements(page_soup)\n",
    "                    cleaned_page_text = text_extractor.extract_main_content(cleaned_page_soup)\n",
    "                    logger.debug(f\"Extracted {len(cleaned_page_text)} characters from {page_type} page\")\n",
    "                    \n",
    "                    # Add to combined text only if we haven't scraped this URL before\n",
    "                    all_text += f\"\\n\\n--- {page_type.upper()} PAGE: {page_url} ---\\n{cleaned_page_text}\"\n",
    "                    scraped_urls.add(page_url)  # Mark this URL as scraped\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error cleaning text for {page_url}: {str(e)}\")\n",
    "                    # Fallback to basic text extraction\n",
    "                    try:\n",
    "                        fallback_text = page_soup.get_text(separator='\\n', strip=True)\n",
    "                        cleaned_page_text = clean_text(fallback_text)\n",
    "                        all_text += f\"\\n\\n--- {page_type.upper()} PAGE: {page_url} ---\\n{cleaned_page_text}\"\n",
    "                        scraped_urls.add(page_url)  # Mark this URL as scraped\n",
    "                        logger.debug(f\"Used fallback text extraction for {page_url}\")\n",
    "                    except Exception:\n",
    "                        logger.error(f\"Failed to extract any text from {page_url}\")\n",
    "                        continue\n",
    "                \n",
    "                # Extract specific information based on page type\n",
    "                if page_type == 'about' and scraped_data_parts['about_page'] is None:\n",
    "                    try:\n",
    "                        scraped_data_parts['about_page'] = cleaned_page_text\n",
    "                        logger.debug(\"Successfully extracted about page content\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error extracting about page: {str(e)}\")\n",
    "                \n",
    "                elif page_type == 'contact':\n",
    "                    try:\n",
    "                        contact_info = extract_contact_info(page_soup, cleaned_page_text)\n",
    "                        if contact_info:\n",
    "                            # Merge contact info, handling list concatenation\n",
    "                            for key, value in contact_info.items():\n",
    "                                if key in scraped_data_parts['contact_info']:\n",
    "                                    # Extend existing lists and remove duplicates\n",
    "                                    scraped_data_parts['contact_info'][key].extend(value)\n",
    "                                    scraped_data_parts['contact_info'][key] = list(dict.fromkeys(\n",
    "                                        scraped_data_parts['contact_info'][key]))\n",
    "                                else:\n",
    "                                    scraped_data_parts['contact_info'][key] = value\n",
    "                            logger.debug(f\"Successfully extracted contact info: {list(contact_info.keys())}\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error extracting contact info: {str(e)}\")\n",
    "                \n",
    "                elif page_type == 'team':\n",
    "                    try:\n",
    "                        team_info = extract_team_info(page_soup)\n",
    "                        if team_info:\n",
    "                            # Add team members, avoiding duplicates by name\n",
    "                            existing_names = {member['name'] for member in scraped_data_parts['team_info']}\n",
    "                            new_members = [member for member in team_info if member['name'] not in existing_names]\n",
    "                            scraped_data_parts['team_info'].extend(new_members)\n",
    "                            logger.debug(f\"Successfully extracted {len(new_members)} new team members\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error extracting team info: {str(e)}\")\n",
    "                \n",
    "                elif page_type == 'products':\n",
    "                    try:\n",
    "                        products = extract_products_services(page_soup)\n",
    "                        if products:\n",
    "                            # Add products, avoiding duplicates by title\n",
    "                            existing_titles = {product['title'] for product in scraped_data_parts['products_services']}\n",
    "                            new_products = [product for product in products if product['title'] not in existing_titles]\n",
    "                            scraped_data_parts['products_services'].extend(new_products)\n",
    "                            logger.debug(f\"Successfully extracted {len(new_products)} new products/services\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error extracting products/services: {str(e)}\")\n",
    "                \n",
    "                elif page_type == 'news':\n",
    "                    try:\n",
    "                        news = extract_news_posts(page_soup)\n",
    "                        if news:\n",
    "                            # Add news items, avoiding duplicates by title\n",
    "                            existing_titles = {item['title'] for item in scraped_data_parts['news_data']}\n",
    "                            new_news = [item for item in news if item['title'] not in existing_titles]\n",
    "                            scraped_data_parts['news_data'].extend(new_news)\n",
    "                            logger.debug(f\"Successfully extracted {len(new_news)} new news items\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error extracting news data: {str(e)}\")\n",
    "                \n",
    "                # Track scraped page (only for unique URLs)\n",
    "                pages_scraped.append({\n",
    "                    'url': page_url,\n",
    "                    'type': page_type,\n",
    "                    'title': page_soup.title.string if page_soup.title else f'{page_type.title()} Page',\n",
    "                    'word_count': len(cleaned_page_text.split()) if 'cleaned_page_text' in locals() else 0\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error scraping {page_url}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Step 7: Create final ScrapedData object\n",
    "        logger.info(\"Step 7: Creating final scraped data object...\")\n",
    "        \n",
    "        # Remove duplicates from team info\n",
    "        if scraped_data_parts['team_info']:\n",
    "            seen_names = set()\n",
    "            unique_team = []\n",
    "            for member in scraped_data_parts['team_info']:\n",
    "                if member['name'] not in seen_names:\n",
    "                    seen_names.add(member['name'])\n",
    "                    unique_team.append(member)\n",
    "            scraped_data_parts['team_info'] = unique_team\n",
    "        \n",
    "        scraped_data = ScrapedData(\n",
    "            company_name=company_name,\n",
    "            description=description,\n",
    "            raw_text=all_text,\n",
    "            team_info=scraped_data_parts['team_info'] if scraped_data_parts['team_info'] else None,\n",
    "            social_links=social_links if social_links else None,\n",
    "            about_page=scraped_data_parts['about_page'],\n",
    "            contact_info=scraped_data_parts['contact_info'] if scraped_data_parts['contact_info'] else None,\n",
    "            products_services=scraped_data_parts['products_services'] if scraped_data_parts['products_services'] else None,\n",
    "            news_data=scraped_data_parts['news_data'] if scraped_data_parts['news_data'] else None,\n",
    "            pages_scraped=pages_scraped,\n",
    "            total_pages_found=len(all_discovered_links)\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Scraping completed successfully!\")\n",
    "        logger.info(f\"- Scraped {len(pages_scraped)} pages out of {len(all_discovered_links)} discovered\")\n",
    "        logger.info(f\"- Total text length: {len(all_text)} characters\")\n",
    "        logger.info(f\"- Company: {company_name}\")\n",
    "        logger.info(f\"- Team members found: {len(scraped_data_parts['team_info']) if scraped_data_parts['team_info'] else 0}\")\n",
    "        logger.info(f\"- Products/services found: {len(scraped_data_parts['products_services']) if scraped_data_parts['products_services'] else 0}\")\n",
    "        logger.info(f\"- Contact info types: {list(scraped_data_parts['contact_info'].keys()) if scraped_data_parts['contact_info'] else []}\")\n",
    "        \n",
    "        return scraped_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error during website scraping: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        raise Exception(f\"Failed to scrape website: {str(e)}\")\n",
    "\n",
    "async def fetch_news_data(company_name: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Fetch external news data about the company\"\"\"\n",
    "    # This is a placeholder for future implementation\n",
    "    # In a real implementation, this would call a news API or web search\n",
    "    logger.debug(f\"fetch_news_data called for {company_name} (placeholder)\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScrapedData(company_name='AI Phone Calls That Convert', description='telli AI agents reach your customers, lead human-like conversations, and complete tasks end-to-end', raw_text=\"--- MAIN PAGE ---\\ntelli agents turn leads into sales opportunities telli AI agents reach your customers, lehuman-like conversations, and complete tasks end-to-end Convert and qualify leads over and messages +12% Con Boappointments in any time zone +33% Boing rate Engage with customers across the entire buying journey Recovered revenue: +k Reach your customers and hand over to human agents Time to first call: <10 sec Reach your customers at the right moment and capture their attention to start the conversation. Lemeaningful, human-like conversations to understand their needs and prepare them for the step. Transition seamlessly to your sales team with warm transfers or scheduled appointments and complete the pre-sales task. Keep every leon track, nurturing, reactivating, and reminding them as they move through their buying journey. You set the rules we follow through. Let telli handle your customer journey Boa demo today Increase pickup rates Increase pickup rates AI that feels real Rely on natural human-quality conversations tailored to each individual customer. Speak with telli Personalize every call Voices in any accent Speak any language Turn every interaction into a win Fill your calendar Boand schedule appointments on the fly during a conversation. Transfer to experts Connect to a sales advisor if available or automatically schedule the best time. Analyze calls at scale Track performance across thousands of calls. telli analyzes sentiment, conversation topics, and outcomesâ\\x80\\x94helping you refine strategies and boost con rates. Kw whatâ\\x80\\x99s happening Calls are transcribed and summarized for quick, actionable insights. Monitor every conversation Monitor and maintain control of every conversation in real time with telli's real-time monitoring. Use any channel Keep customers engaged through , SMS, and WhatsApp throughout their buying journey. 5 p.p. boing con through time savings of sales people 50% faster response times with potential customers â\\x80\\x9cSince we started using telli, our sales floor has gone from chaos to calm. The team w focuses on helping customers insteof dealing with the hassle of reaching and scheduling themâ\\x80\\x94and we havenâ\\x80\\x99t missed a single lesince. Itâ\\x80\\x99s been a game-changer.â\\x80\\x9d Richard Ruben CEO & Co-Founder, KitchenAdvisor 3Ã\\x97 increase in customer engagement â\\x80\\x9cThe telli agent is a valuable addition to our teamâ\\x80\\x94keeping queue lengths low during peak times, engaging with potential buyers in in-depth conversations about the property and boing highly qualified viewing appointments for our local agentsâ\\x80\\x9d Mathias Kche COO, McMakler 100% boing request coverage without adding operation overheâ\\x80\\x9ctelli was up and running in our tenant appointment boing process within just a few weeks â\\x80\\x94 the onboarding was lightning-fast. Thanks to telli, weâ\\x80\\x99ve kept our operations incredibly lean, while still being able to answer complex questions and requests that go beyond just boing appointments.â\\x80\\x9d Yannick Klinkenberg Heof Operations, termios About Us telli is the AI call automation platform. Our team developed software products, customer processes, and strategies for leading companies. w we are reimagining your customers buying journey. Join us Built by people from We hor your trust. Your trust is our priority. We safeguard your data with the highest standards of security and compliance, ensuring every interaction is protected and every user feels secure.\", team_info=None, social_links=None, about_page=None, contact_info=None, products_services=None, news_data=None, pages_scraped=[{'url': 'https://telli.com', 'type': 'main', 'title': \"AI Phone Calls That Convert | telli's AI Sales Agent & Auto Dialer\", 'word_count': 507}], total_pages_found=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_data = await scrape_website(\"https://telli.com\", max_pages=5)\n",
    "scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
